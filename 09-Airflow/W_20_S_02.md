# Session 2

## Dependencies between tasks
```python
>>> task1 >> [task2, task3]
```

## Lifecycle of a task
- **Scheduled :** Schedule the next tasks if a condition/ trigger, else skips.
- **Upstream Failed :** If one of the tasks failed before others are executed.
     ```python
     >>>from airflow import DAG
     >>>from airflow.utils.dates import days_ago
     >>>from airflow.operators.bash_operator import BashOperator
     >>>dag = DAG(dag_id="helloworld_dag", schedule_interval='@daily', start_date=days_ago(1))
     >>>task1 = BashOperator(task_id = 't1', bash_command='eacho hello && exit 1', dag = dag)
     >>>task2 = BashOperator(task_id = 't2', bash_command='eacho t2', dag = dag)
     >>>task3 = BashOperator(task_id = 't3', bash_command='eacho t3', dag = dag)
     >>>task1 >> [task2, task3]
     ```
- **up_for_reschedule :** If one of the tasks failed it will be marked for reschudle.
- **up_for_retry :** No of retries, if failed.
    ```python
     >>>from airflow import DAG
     >>>from airflow.utils.dates import days_ago
     >>>from airflow.operators.bash_operator import BashOperator
     >>>dag = DAG(dag_id="helloworld_dag", schedule_interval='@daily', start_date=days_ago(1))
     >>>task1 = BashOperator(task_id = 't1', bash_command='eacho hello && exit 1', dag = dag, retries=3)
     >>>task2 = BashOperator(task_id = 't2', bash_command='eacho t2', dag = dag)
     >>>task3 = BashOperator(task_id = 't3', bash_command='eacho t3', dag = dag)
     >>>task1 >> [task2, task3]
     ```
- **failed :**
- **success :**
- **running :**
- **queued :**
- **No status :**

## Trigger Rule: Applicable only on child not on parent tasks.
**Problem Statement :** Execute T2 if T1 is success else Execute T3
1. all_success
2. all_failed
3. all_done: All the previous tasks are done, i'll run doesn't matter if tasks failes or sucess.
4. one_failed: [t1, t2] >> t3. If either of t1 & t2 is failed t3 is triggered.
5. one_success:
6. none_failed: All parents tasks are either in success or skipped state.
7. none_failed_or_skipped: All parents tasks are not failed and atleast one is success or skipped state
8. none_skipped:
9. dummy:
all_success is the default trigger rule.

### Examples
1. all_failed: task3 is executed
    ```python
     >>>from airflow import DAG
     >>>from airflow.utils.dates import days_ago
     >>>from airflow.operators.bash_operator import BashOperator
     >>>dag = DAG(dag_id="helloworld_dag", schedule_interval='@daily', start_date=days_ago(1))
     >>>task1 = BashOperator(task_id = 't1', bash_command='eacho hello && exit 1', dag = dag)
     >>>task2 = BashOperator(task_id = 't2', bash_command='eacho t2', dag = dag)
     >>>task3 = BashOperator(task_id = 't3', bash_command='eacho t3', dag = dag, trigger_rule='all_failed')
     >>>task1 >> [task2, task3]
     ```
2. all_success: task1 and task2 will be executed.task3 goes into skipped state.
    ```python
     >>>from airflow import DAG
     >>>from airflow.utils.dates import days_ago
     >>>from airflow.operators.bash_operator import BashOperator
     >>>dag = DAG(dag_id="helloworld_dag", schedule_interval='@daily', start_date=days_ago(1))
     >>>task1 = BashOperator(task_id = 't1', bash_command='eacho hello', dag = dag)
     >>>task2 = BashOperator(task_id = 't2', bash_command='eacho t2', dag = dag)
     >>>task3 = BashOperator(task_id = 't3', bash_command='eacho t3', dag = dag, trigger_rule='all_failed')
     >>>task1 >> [task2, task3]
     ```